{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph as knng\n",
    "from scipy.sparse import load_npz\n",
    "# with open('data/paragraphs.data','rb') as f :\n",
    "#     doc_model = pickle.load(f ) \n",
    "# with open('data/paragraphs.data.docvecs.vectors_docs.npy','rb') as f_arr :\n",
    "#     doc_array = np.load(f_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_doc(log=False):\n",
    "    with open(\"data/history_depth3.csv\", 'r', encoding='utf-8') as file:\n",
    "        csv_file = pandas.read_csv(file)\n",
    "    i = 0\n",
    "    to_list = []\n",
    "    for name, pageid in zip(csv_file[\"title\"], csv_file[\"pageid\"]):\n",
    "        try:\n",
    "            yield doc_model.docvecs.doctags[str(pageid)].offset, name\n",
    "        except:\n",
    "            i += 1\n",
    "            if log:\n",
    "                to_list.append(str(x))\n",
    "    print(i)\n",
    "    if log:\n",
    "        print(to_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15993\n"
     ]
    }
   ],
   "source": [
    "G = nx.from_scipy_sparse_matrix(result, create_using=nx.DiGraph())\n",
    "labels = {k: v for k, v in get_all_doc()}\n",
    "nx.relabel_nodes(G, labels, copy=False)\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_as_gexf(name=300):\n",
    "    result = load_npz(f\"test/graph_reduced_{name}.npz\")\n",
    "    G = nx.from_scipy_sparse_matrix(result, create_using=nx.DiGraph())\n",
    "    del result\n",
    "    labels = {k: v for k, v in enumerate(np.load(f\"test/labels_{name}.npy\"))}\n",
    "    labels_rev = {v: k for k, v in labels.items()}\n",
    "    nx.relabel_nodes(G, labels, copy=False)\n",
    "    nx.set_node_attributes(G, labels_rev,'id')\n",
    "    nx.write_gexf(G, f\"graphs/graph_reduced{i}.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_to_csv(graph, labels, pagerank=None, betweenness_centrality=None, log=False):\n",
    "    i = 0\n",
    "    to_list = []\n",
    "#     if pagerank == None:\n",
    "#         pagerank = nx.pagerank(graph)\n",
    "#     if betweenness_centrality == None:\n",
    "#         betweenness_centrality = nx.betweenness_centrality(graph)\n",
    "#     yield 'label,', 'pagerank,', 'betweenness' \n",
    "#     yield 'string,', 'real,', 'real'\n",
    "    for key, value in sorted(labels.items(), key=lambda x: x[0]):\n",
    "        try:\n",
    "            yield key, value#, pagerank(key), betweenness_centrality(key) , 'pagerank', 'betweenness_centrality'\n",
    "        except:\n",
    "            i += 1\n",
    "            if log:\n",
    "                to_list.append(str(x))\n",
    "    print(i)\n",
    "    if log:\n",
    "        print(to_list)\n",
    "def save_as_csv(graph, labels, i):\n",
    "    nx.write_edgelist(graph, f'graph_data\\edges_{i}.csv', delimiter='\\t', data=False)\n",
    "    pandas.DataFrame(get_to_csv(graph, labels),columns=['id', 'label']).to_csv(f'nodes_{i}.csv', sep='\\t', encoding='utf-8', index=False, header=False)\n",
    "    with open(f'nodes_{i}.csv', 'r+', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        text = 'name\\nstring\\n'\n",
    "        f.seek(0, 0)\n",
    "        f.write(text + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def save_to_special_program(name=300):\n",
    "    result = load_npz(f\"test/graph_reduced_{name}.npz\")\n",
    "    G = nx.from_scipy_sparse_matrix(result, create_using=nx.DiGraph())\n",
    "    del result\n",
    "    labels = {k: v for k, v in enumerate(np.load(f\"test/labels_{name}.npy\"))}\n",
    "    save_as_csv(G, labels, name)\n",
    "\n",
    "#for i in range(30, 401, 30):\n",
    "#    save_as_gexf(name=i)\n",
    "for i in range(5, 30, 10):\n",
    "    save_to_special_program(name=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In edges:\n",
      "\tAlbert_Speer -> Adolf_Hitler\n",
      "\tAdolf_Hitler's_rise_to_power -> Adolf_Hitler\n",
      "\tReligious_views_of_Adolf_Hitler -> Adolf_Hitler\n",
      "\tHitler_family -> Adolf_Hitler\n",
      "\tWehrmacht -> Adolf_Hitler\n",
      "Out edges:\n",
      "\tAdolf_Hitler -> Albert_Speer\n",
      "\tAdolf_Hitler -> Wannsee_Conference\n",
      "\tAdolf_Hitler -> Night_of_the_Long_Knives\n",
      "\tAdolf_Hitler -> Madagascar_Plan\n",
      "\tAdolf_Hitler -> Frederick_Taylor_(historian)\n",
      "\tAdolf_Hitler -> Adolf_Hitler's_rise_to_power\n",
      "\tAdolf_Hitler -> Religious_views_of_Adolf_Hitler\n",
      "\tAdolf_Hitler -> Reich_Chancellery_meeting_of_12_December_1941\n",
      "\tAdolf_Hitler -> Wehrmacht\n",
      "\tAdolf_Hitler -> Lieberose_forced_labor_camp\n"
     ]
    }
   ],
   "source": [
    "def print_edges(name=None):\n",
    "    if not name:\n",
    "        name = \"Adolf_Hitler\"\n",
    "    print(\"In edges:\")\n",
    "    print(\"\\n\".join(map(lambda t: f\"\\t{t[0]} -> {t[1]}\", G.in_edges(name))))\n",
    "    print(\"Out edges:\")\n",
    "    print(\"\\n\".join(map(lambda t: f\"\\t{t[0]} -> {t[1]}\", G.out_edges(name))))\n",
    "print_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In edges:\n",
      "\tAftermath_of_World_War_I -> World_War_II\n",
      "\tCold_War -> World_War_II\n",
      "\tOrigins_of_the_Cold_War -> World_War_II\n",
      "\tMobilization -> World_War_II\n",
      "\tCauses_of_World_War_II -> World_War_II\n",
      "\tMiddle_East_Theatre_of_World_War_II -> World_War_II\n",
      "\tMediterranean_and_Middle_East_theatre_of_World_War_II -> World_War_II\n",
      "\tJapan_during_World_War_I -> World_War_II\n",
      "\tWorld_War_I -> World_War_II\n",
      "\tAftermath_of_World_War_II -> World_War_II\n",
      "Out edges:\n",
      "\tWorld_War_II -> Potsdam_Conference\n",
      "\tWorld_War_II -> Potsdam_Agreement\n",
      "\tWorld_War_II -> 196\n",
      "\tWorld_War_II -> Tehran_Conference\n",
      "\tWorld_War_II -> Cairo_Conference\n",
      "\tWorld_War_II -> Events_preceding_World_War_II_in_Europe\n",
      "\tWorld_War_II -> Middle_East_Theatre_of_World_War_II\n",
      "\tWorld_War_II -> Mediterranean_and_Middle_East_theatre_of_World_War_II\n",
      "\tWorld_War_II -> Japan_during_World_War_I\n",
      "\tWorld_War_II -> 1126_in_art\n"
     ]
    }
   ],
   "source": [
    "print_edges(\"World_War_II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_max_degree(n=10):\n",
    "    print(f\"{n} highest degree:\")\n",
    "    print(\", \".join(map(lambda t: t[1], sorted([(d,n) for n, d in G.degree()], reverse=True, key=lambda t: t[0])[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 highest degree:\n",
      "426, AD_32, 264, 112, Raczkiewicz, 140, AD_29, 99_BC, AD_61, 125, 144, 402_BC, 318, AD_37, AD_41, 2024, AD_31, 817, Mohammed_al-Duayf, Ten'ō, AD_76, 451, 787, AD_20, 373_BC, Suzuko_no_Koi, Rizvan, Bogić, Emergent_materialism, Pejo, 12_BC, Declaration_of_Indulgence, AD_19, December_7, AD_11, 537_BC, Ryujoseph, 278, Chilean_presidential_election,_1856, List_of_peers_1320–1329, Emigrant_Gap, 233, Plasma_processing, List_of_years_in_Greece, 349, 520_BC, 141, Bezruč, 517, 93_BC, United_Nations_Security_Council_Resolution_584, Fidan, 781, Pretoria_Accord, Empirical_knowledge, Sagar,_Madhya_Pradesh, Kadelburg, 816, 772, Hisham_ibn_al-Kalbi, Chang-woo, February_7, Japan_Coast_Guard_Museum, United_Nations_Security_Council_Resolution_735, 304, The_Love_of_Richard_Nixon, Šolontu, Cisapride, 136_BC, Self-evidence, Long_knives, Tuo, 1380_in_art, 754, Issaka, Jess, Web_browser, 2066, 1527_in_music, Hatogaya_Domain, Noubar, February_1, 401_BC, Pulseless_electrical_activity, Svetozar, From_Beirut_to_Jerusalem, Burshtyn, Sheikh_Hamdullah, 1970_in_paleontology, 757, 1177_in_Norway, Thomas_Egerton,_1st_Viscount_Brackley, 843, 474_BC, Aperture_grille, Gonzalo_Pizarro, Michio, Negroponte_doctrine, 620, 405_BC, Itraconazole, Jula_(name), September_4, Czochralski_process, Partmaximum, Arthur_Stanley_Tritton, Baby_Butch, December_28, Pachat'aqa, Commonwealth_of_England, Cumene_process, 1965_in_paleontology, Abdul_Haq_Choudhury, Refreshable_braille_display, AD_26, Black_dwarf, Valerie_Perrine, Levetiracetam, Shmuel_Yeivin, Thiha, Indictable_offence, Spray_drying, Francisco_de_Montejo, List_of_Japanese_armored_divisions, Cleombrotus_(regent), 559, Topiramate, Herzl, Shao_Prefecture, Bunichiro, List_of_hoards_in_Romania, List_of_years_in_Lebanon, Monsanto_process, Marko_Atlagić, Peroxide_process, Countersink, Reason_and_Revolution, Grzegorz_Bębnik, George_Goring,_1st_Earl_of_Norwich, Alexandru_Moșanu, AD_22, 1423, 2077, 2002_Arab_League_summit, Hydrogen_analyzer, Timeline_of_Peruvian_history, Yuan_Hong_(Jin_dynasty), Svetlana_Pletnyova, Marxist_international_relations_theory, The_Hazara_People_and_Greater_Khorasan\n"
     ]
    }
   ],
   "source": [
    "n_max_degree(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9361\n"
     ]
    }
   ],
   "source": [
    "print(len(list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('data/doc_length.data', 'rb') as f:\n",
    "    art_lens = pickle.load(f)\n",
    "def reduce_hi_de(num=300):\n",
    "    for pageid, size in art_lens:\n",
    "        if size < num:\n",
    "            yield doc_model.docvecs.doctags[str(pageid)].offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117977, 300)\n"
     ]
    }
   ],
   "source": [
    "print((doc_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph as knng\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15993\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([v for k, v in get_all_doc()])\n",
    "for i in range(30, 401, 30):\n",
    "    to_delete = sorted(list(reduce_hi_de(i)))\n",
    "    new_array = np.delete(doc_array,to_delete , axis=0)\n",
    "    new_labels = np.delete(labels, to_delete, axis=0)\n",
    "    np.save(f\"test/labels_{i}\", new_labels)\n",
    "    del new_labels\n",
    "    del to_delete\n",
    "    X = normalize(new_array)    \n",
    "    result = knng(X, n_neighbors=10, n_jobs=-1, mode='distance')\n",
    "    save_npz(f\"test/graph_reduced_{i}\", result)\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5, 30, 10):\n",
    "    to_delete = sorted(list(reduce_hi_de(i)))\n",
    "    new_array = np.delete(doc_array,to_delete , axis=0)\n",
    "    new_labels = np.delete(labels, to_delete, axis=0)\n",
    "    np.save(f\"test/labels_{i}\", new_labels)\n",
    "    del new_labels\n",
    "    del to_delete\n",
    "    X = normalize(new_array)    \n",
    "    result = knng(X, n_neighbors=10, n_jobs=-1, mode='distance')\n",
    "    save_npz(f\"test/graph_reduced_{i}\", result)\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
