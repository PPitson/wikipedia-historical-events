{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph as knng\n",
    "from scipy.sparse import load_npz\n",
    "from gensim.models import Doc2Vec\n",
    "VECOTORS = 'paragraphs50.data.trainables.syn1neg.npy'\n",
    "DATA = 'paragraphs50.data'\n",
    "doc_model = Doc2Vec.load('data/paragraphs50.data')\n",
    "# with open(f\"data/{DATA}\",'rb') as f:\n",
    "#     doc_model = pickle.load(f ) \n",
    "# with open(f\"data/{VECOTORS}\",'rb') as f_arr :\n",
    "doc_array = doc_model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117977, 50)\n"
     ]
    }
   ],
   "source": [
    "print(doc_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_doc(log=False):\n",
    "    with open(\"data/history_depth3.csv\", 'r', encoding='utf-8') as file:\n",
    "        csv_file = pandas.read_csv(file)\n",
    "    i = 0\n",
    "    to_list = []\n",
    "    for name, pageid in zip(csv_file[\"title\"], csv_file[\"pageid\"]):\n",
    "        try:\n",
    "            yield doc_model.docvecs.doctags[str(pageid)].offset, name\n",
    "        except:\n",
    "            i += 1\n",
    "            if log:\n",
    "                to_list.append(str(x))\n",
    "    print(i)\n",
    "    if log:\n",
    "        print(to_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_as_gexf(name=300):\n",
    "    result = load_npz(f\"test/graph_reduced_{name}.npz\")\n",
    "    G = nx.from_scipy_sparse_matrix(result, create_using=nx.DiGraph())\n",
    "    del result\n",
    "    labels = {k: v for k, v in enumerate(np.load(f\"test/labels_{name}.npy\"))}\n",
    "    labels_rev = {v: k for k, v in labels.items()}\n",
    "    nx.relabel_nodes(G, labels, copy=False)\n",
    "    nx.set_node_attributes(G, labels_rev,'id')\n",
    "    nx.write_gexf(G, f\"graphs/graph_reduced{i}.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_to_csv(graph, labels):\n",
    "    for key, value in sorted(labels.items(), key=lambda x: x[0]):\n",
    "        yield key, float(graph.degree(key)), value.replace('_', ' ').encode('ascii',errors='ignore')\n",
    "\n",
    "def save_as_csv(graph, labels, i, weight = ['weight']):\n",
    "#     nx.write_edgelist(graph, f'{i}.graph', delimiter='\\t', data=False, encoding='utf-8')\n",
    "    with open( f'{i}.graph', 'w', encoding='ascii') as f:\n",
    "        for line in nx.generate_edgelist(graph, data=False):\n",
    "            f.write(line+'\\n')\n",
    "    pandas.DataFrame(get_to_csv(graph, labels),columns=['id', 'name', 'degree']).to_csv(f'{i}.desc', sep='\\t', encoding='ascii', index=False, header=False)\n",
    "    with open(f'{i}.desc', 'r+', encoding='ascii') as f:\n",
    "        content = f.read()\n",
    "        text = 'degree,\\ttitle\\t\\nreal,\\tstring \\n'\n",
    "        f.seek(0, 0)\n",
    "        f.write(text + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test/graph_reduced_5.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-52b6ac6e6cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msave_to_special_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msave_to_special_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-52b6ac6e6cf9>\u001b[0m in \u001b[0;36msave_to_special_program\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_to_special_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"test/graph_reduced_{name}.npz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_scipy_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_using\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"test/labels_{name}.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_matrix_io.py\u001b[0m in \u001b[0;36mload_npz\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mPICKLE_KWARGS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mmatrix_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'format'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test/graph_reduced_5.npz'"
     ]
    }
   ],
   "source": [
    "def save_to_special_program(name=300):\n",
    "    result = load_npz(f\"test/graph_reduced_{name}.npz\")\n",
    "    G = nx.from_scipy_sparse_matrix(result, create_using=nx.DiGraph())\n",
    "    del result\n",
    "    labels = {k: v for k, v in enumerate(np.load(f\"test/labels_{name}.npy\"))}\n",
    "    save_as_csv(G, labels, name)\n",
    "\n",
    "for i in range(30, 401, 30):\n",
    "    save_to_special_program(name=i)\n",
    "for i in range(5, 30, 10):\n",
    "    save_to_special_program(name=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In edges:\n",
      "\tAlbert_Speer -> Adolf_Hitler\n",
      "\tAdolf_Hitler's_rise_to_power -> Adolf_Hitler\n",
      "\tReligious_views_of_Adolf_Hitler -> Adolf_Hitler\n",
      "\tHitler_family -> Adolf_Hitler\n",
      "\tWehrmacht -> Adolf_Hitler\n",
      "Out edges:\n",
      "\tAdolf_Hitler -> Albert_Speer\n",
      "\tAdolf_Hitler -> Wannsee_Conference\n",
      "\tAdolf_Hitler -> Night_of_the_Long_Knives\n",
      "\tAdolf_Hitler -> Madagascar_Plan\n",
      "\tAdolf_Hitler -> Frederick_Taylor_(historian)\n",
      "\tAdolf_Hitler -> Adolf_Hitler's_rise_to_power\n",
      "\tAdolf_Hitler -> Religious_views_of_Adolf_Hitler\n",
      "\tAdolf_Hitler -> Reich_Chancellery_meeting_of_12_December_1941\n",
      "\tAdolf_Hitler -> Wehrmacht\n",
      "\tAdolf_Hitler -> Lieberose_forced_labor_camp\n"
     ]
    }
   ],
   "source": [
    "def print_edges(name=None):\n",
    "    if not name:\n",
    "        name = \"Adolf_Hitler\"\n",
    "    print(\"In edges:\")\n",
    "    print(\"\\n\".join(map(lambda t: f\"\\t{t[0]} -> {t[1]}\", G.in_edges(name))))\n",
    "    print(\"Out edges:\")\n",
    "    print(\"\\n\".join(map(lambda t: f\"\\t{t[0]} -> {t[1]}\", G.out_edges(name))))\n",
    "print_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In edges:\n",
      "\tAftermath_of_World_War_I -> World_War_II\n",
      "\tCold_War -> World_War_II\n",
      "\tOrigins_of_the_Cold_War -> World_War_II\n",
      "\tMobilization -> World_War_II\n",
      "\tCauses_of_World_War_II -> World_War_II\n",
      "\tMiddle_East_Theatre_of_World_War_II -> World_War_II\n",
      "\tMediterranean_and_Middle_East_theatre_of_World_War_II -> World_War_II\n",
      "\tJapan_during_World_War_I -> World_War_II\n",
      "\tWorld_War_I -> World_War_II\n",
      "\tAftermath_of_World_War_II -> World_War_II\n",
      "Out edges:\n",
      "\tWorld_War_II -> Potsdam_Conference\n",
      "\tWorld_War_II -> Potsdam_Agreement\n",
      "\tWorld_War_II -> 196\n",
      "\tWorld_War_II -> Tehran_Conference\n",
      "\tWorld_War_II -> Cairo_Conference\n",
      "\tWorld_War_II -> Events_preceding_World_War_II_in_Europe\n",
      "\tWorld_War_II -> Middle_East_Theatre_of_World_War_II\n",
      "\tWorld_War_II -> Mediterranean_and_Middle_East_theatre_of_World_War_II\n",
      "\tWorld_War_II -> Japan_during_World_War_I\n",
      "\tWorld_War_II -> 1126_in_art\n"
     ]
    }
   ],
   "source": [
    "print_edges(\"World_War_II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_max_degree(n=10):\n",
    "    print(f\"{n} highest degree:\")\n",
    "    print(\", \".join(map(lambda t: t[1], sorted([(d,n) for n, d in G.degree()], reverse=True, key=lambda t: t[0])[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 highest degree:\n",
      "426, AD_32, 264, 112, Raczkiewicz, 140, AD_29, 99_BC, AD_61, 125, 144, 402_BC, 318, AD_37, AD_41, 2024, AD_31, 817, Mohammed_al-Duayf, Ten'ō, AD_76, 451, 787, AD_20, 373_BC, Suzuko_no_Koi, Rizvan, Bogić, Emergent_materialism, Pejo, 12_BC, Declaration_of_Indulgence, AD_19, December_7, AD_11, 537_BC, Ryujoseph, 278, Chilean_presidential_election,_1856, List_of_peers_1320–1329, Emigrant_Gap, 233, Plasma_processing, List_of_years_in_Greece, 349, 520_BC, 141, Bezruč, 517, 93_BC, United_Nations_Security_Council_Resolution_584, Fidan, 781, Pretoria_Accord, Empirical_knowledge, Sagar,_Madhya_Pradesh, Kadelburg, 816, 772, Hisham_ibn_al-Kalbi, Chang-woo, February_7, Japan_Coast_Guard_Museum, United_Nations_Security_Council_Resolution_735, 304, The_Love_of_Richard_Nixon, Šolontu, Cisapride, 136_BC, Self-evidence, Long_knives, Tuo, 1380_in_art, 754, Issaka, Jess, Web_browser, 2066, 1527_in_music, Hatogaya_Domain, Noubar, February_1, 401_BC, Pulseless_electrical_activity, Svetozar, From_Beirut_to_Jerusalem, Burshtyn, Sheikh_Hamdullah, 1970_in_paleontology, 757, 1177_in_Norway, Thomas_Egerton,_1st_Viscount_Brackley, 843, 474_BC, Aperture_grille, Gonzalo_Pizarro, Michio, Negroponte_doctrine, 620, 405_BC, Itraconazole, Jula_(name), September_4, Czochralski_process, Partmaximum, Arthur_Stanley_Tritton, Baby_Butch, December_28, Pachat'aqa, Commonwealth_of_England, Cumene_process, 1965_in_paleontology, Abdul_Haq_Choudhury, Refreshable_braille_display, AD_26, Black_dwarf, Valerie_Perrine, Levetiracetam, Shmuel_Yeivin, Thiha, Indictable_offence, Spray_drying, Francisco_de_Montejo, List_of_Japanese_armored_divisions, Cleombrotus_(regent), 559, Topiramate, Herzl, Shao_Prefecture, Bunichiro, List_of_hoards_in_Romania, List_of_years_in_Lebanon, Monsanto_process, Marko_Atlagić, Peroxide_process, Countersink, Reason_and_Revolution, Grzegorz_Bębnik, George_Goring,_1st_Earl_of_Norwich, Alexandru_Moșanu, AD_22, 1423, 2077, 2002_Arab_League_summit, Hydrogen_analyzer, Timeline_of_Peruvian_history, Yuan_Hong_(Jin_dynasty), Svetlana_Pletnyova, Marxist_international_relations_theory, The_Hazara_People_and_Greater_Khorasan\n"
     ]
    }
   ],
   "source": [
    "n_max_degree(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9361\n"
     ]
    }
   ],
   "source": [
    "print(len(list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('data/doc_length.data', 'rb') as f:\n",
    "    art_lens = pickle.load(f)\n",
    "def reduce_hi_de(num=300):\n",
    "    for pageid, size in art_lens:\n",
    "        if size < num:\n",
    "            try:\n",
    "                yield doc_model.docvecs.doctags[str(pageid)].offset\n",
    "            except:\n",
    "                continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117977, 50)\n",
      "117977\n"
     ]
    }
   ],
   "source": [
    "print((doc_array.shape))\n",
    "print(len(art_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph as knng\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15993\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([v for k, v in get_all_doc()])\n",
    "for i in range(30, 401, 30):\n",
    "    to_delete = sorted(list(reduce_hi_de(i)))\n",
    "    new_array = np.delete(doc_array,to_delete , axis=0)\n",
    "    new_labels = np.delete(labels, to_delete, axis=0)\n",
    "    np.save(f\"test/labels_{i}\", new_labels)\n",
    "    del new_labels\n",
    "    del to_delete\n",
    "    X = normalize(new_array)    \n",
    "    result = knng(X, n_neighbors=10, n_jobs=4, mode='distance')\n",
    "    save_npz(f\"test/graph_reduced_{i}\", result)\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5, 30, 10):\n",
    "    to_delete = sorted(list(reduce_hi_de(i)))\n",
    "    new_array = np.delete(doc_array,to_delete , axis=0)\n",
    "    new_labels = np.delete(labels, to_delete, axis=0)\n",
    "    np.save(f\"test/TEST/labels_{i}\", new_labels)\n",
    "    del new_labels\n",
    "    del to_delete\n",
    "    X = normalize(new_array)    \n",
    "    result = knng(X, n_neighbors=10, n_jobs=4, mode='distance')\n",
    "    save_npz(f\"test/TEST/graph_reduced_{i}\", result)\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- wizualizować w 2d\n",
    "- wziąć co 10 punkt, żeby było cokolwiek widać\n",
    "- bounding box nie potrzebny\n",
    "- kolorowanie zgodnie ze stopniem wierzchołków, wchodzcacych  lub  wychodzących\n",
    "\t- algorithm : full, \n",
    "\t- random neighbours : 2 , \n",
    "\t- speed factor: 100, można tym sterować, to o jakąś zbieżność chodzi\n",
    "\t- kfactor for rand = 0.01m \n",
    "- na dole przyciski można sterować widokiem\n",
    "- settings = przeźroczytośc krawędzi,  ustawić w miarę transparent, żeby lepiej było widać\n",
    "- max sphere size - diff pomiędzy min  a max  rozmiarem kulki, trzeba kliknąć potem  coś ,bo samo  się  nie zaktualizuje\n",
    "-  shit + kliknąć  - pokaże info o wierzchołku\n",
    "- dobrze jest zrobić zoom i zmniejszyć skale i wtedy coś lepiej widać\n",
    "- przefiltrować  po degree i schować, te z mniejszym degree\n",
    "- shift+ zaznać obszar można eksportowac kawałek grafu\n",
    "- select connected - pokaz najbliższych sąsiadów\n",
    "- plik: .graph - tsv z listą połączeń\n",
    " - vector size ,  wypróbować kilka różnych 50, 100, 150, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
